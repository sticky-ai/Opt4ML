{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "\n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "\n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자전거 대여 횟수 예측하기\n",
    "\n",
    "2011년에서 2012년도 미국의 워싱턴 DC의 자전거 대여 공공데이터를 사용한다.[^DCbicycle]\n",
    "자료에는 날짜, 날씨, 온도, 계절, 체감온도, 풍속, 자전거 대여 횟수 등이 시간별로 기록되어 있다.\n",
    "\n",
    "[^DCbicycle]: http://capitalbikeshare.com/system-data\n",
    "\n",
    "> Fanaee-T, Hadi, and Gama, Joao, \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3\n",
    "\n",
    "자전거 대여 횟수를 예측하기 위해, 신경망 모델(Neural Network Model)을 사용한 회귀(Regression) 분석을 하는 것이 이번 프로젝트의 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 훑어보기\n",
    "\n",
    "제공된 `hour.csv` 파일은 다음과 같은 구조를 갖고 있다. 다음은 앞 10개의 행들이다.\n",
    "\n",
    "| instant | dteday     | season | yr | mnth | hr | holiday | weekday | workingday | weathersit | temp | atemp  | hum  | windspeed | casual | registered | cnt | \n",
    "|---------|------------|--------|----|------|----|---------|---------|------------|------------|------|--------|------|-----------|--------|------------|-----| \n",
    "| 1       | 2011-01-01 | 1      | 0  | 1    | 0  | 0       | 6       | 0          | 1          | 0.24 | 0.2879 | 0.81 | 0         | 3      | 13         | 16  | \n",
    "| 2       | 2011-01-01 | 1      | 0  | 1    | 1  | 0       | 6       | 0          | 1          | 0.22 | 0.2727 | 0.8  | 0         | 8      | 32         | 40  | \n",
    "| 3       | 2011-01-01 | 1      | 0  | 1    | 2  | 0       | 6       | 0          | 1          | 0.22 | 0.2727 | 0.8  | 0         | 5      | 27         | 32  | \n",
    "| 4       | 2011-01-01 | 1      | 0  | 1    | 3  | 0       | 6       | 0          | 1          | 0.24 | 0.2879 | 0.75 | 0         | 3      | 10         | 13  | \n",
    "| 5       | 2011-01-01 | 1      | 0  | 1    | 4  | 0       | 6       | 0          | 1          | 0.24 | 0.2879 | 0.75 | 0         | 0      | 1          | 1   | \n",
    "| 6       | 2011-01-01 | 1      | 0  | 1    | 5  | 0       | 6       | 0          | 2          | 0.24 | 0.2576 | 0.75 | 0.0896    | 0      | 1          | 1   | \n",
    "| 7       | 2011-01-01 | 1      | 0  | 1    | 6  | 0       | 6       | 0          | 1          | 0.22 | 0.2727 | 0.8  | 0         | 2      | 0          | 2   | \n",
    "| 8       | 2011-01-01 | 1      | 0  | 1    | 7  | 0       | 6       | 0          | 1          | 0.2  | 0.2576 | 0.86 | 0         | 1      | 2          | 3   | \n",
    "| 9       | 2011-01-01 | 1      | 0  | 1    | 8  | 0       | 6       | 0          | 1          | 0.24 | 0.2879 | 0.75 | 0         | 1      | 7          | 8   | \n",
    "| 10      | 2011-01-01 | 1      | 0  | 1    | 9  | 0       | 6       | 0          | 1          | 0.32 | 0.3485 | 0.76 | 0         | 8      | 6          | 14  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 코드를 통해, `hour.csv`파일을 `pandas`의 `read_csv()`를 사용하여 `rides`라는 `DataFrame`을 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "rides = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rides`의 각 열별로 나타내는 정보는 다음과 같다.\n",
    "\n",
    "* `instant`: 기록번호\n",
    "* `dteday`: 날짜\n",
    "* `season`: 계절 (`1`:봄, `2`:여름, `3`:가을, `4`:겨울)\n",
    "* `yr`: 연도 (`0`: 2011, 1:2012)\n",
    "* `mnth`: 달 (`1` ~ `12`)\n",
    "* `hr`: 시간 (`0` ~ `23`)\n",
    "* `holiday`: 휴일(`0`/`1`)\n",
    "* `weekday`: 요일\n",
    "* `workingday`: 평일(`0`/`1`)\n",
    "* `weathersit`:\n",
    "    * `1`: 맑거나 조금 흐림\n",
    "    * `2`: 안개 및 흐림\n",
    "    * `3`: 가벼운 눈, 비, 뇌우\n",
    "    * `4`: 많은 비, 눈, 뇌우\n",
    "* `temp`: `0`에서 `1`까지의 표준화된 섭씨 온도(최저기온 : -8, 최고기온 : 39)\n",
    "* `atemp`: `0`에서 `1`까지의 표준화된 섭씨 체감온도(최저기온 : -16, 최고기온 : 50)\n",
    "* `hum`: `0`에서 `1`까지 표준화된 습도(예: 0.3 = 30%)\n",
    "* `windspeed`: 표준화된 풍속(최고속도 : 67)\n",
    "* `casual`: 비회원의 자전거 대여횟수\n",
    "* `registered`: 등록회원의 자전거 대여횟수\n",
    "* `cnt`: 비회원 및 등록회원의 총 자전거 대여횟수\n",
    "\n",
    "앞서 언급했듯이 자전거 대여 횟수를 예측하는 신경망 회귀 모델을 찾는 것이 이번 장에서의 목표이다. 대여 횟수가 총 3가지 종류가 있다. 그중에 비회원과 등록회원 모두의 자전거 대여 횟수인 `cnt`가 예측 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 첫 10일의 데이터를 날짜/총대여량을 나타내주는 그래프이다. 아주 이른 새벽에는 대여량이 급격히 줄어드는 것을 볼 수 있다. 신경망 모델을 통해 아래 그래프와 비슷한 예측 그래프를 확보할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides[:24*10].plot(x='dteday', y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정돈하기\n",
    "### Dummy 변수 도입하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before applying dummy variables : \")\n",
    "print(sorted(rides.columns.values))\n",
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "print(\"\")\n",
    "print(\"After applying dummy variables : \")\n",
    "print(sorted(rides.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 Feature 선택하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주요한 핵심 요소들만 추려서 모델을 만드는 것이 좋다.\n",
    "다음 코드는 앞서 제거하기로 결정한 5가지 항목에 대해서 `df.drop()`을 사용하여 해당 열을 지우는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_drop = ['instant', 'dteday', 'season', 'atemp', 'workingday']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "print(sorted(rides.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 표준화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Validation 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for approximately the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "\n",
    "# Now remove the test data from the data set \n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets = features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 목표\n",
    "\n",
    "* `curr_loss`값이 `0.05` 밑으로 내려가는 것을 목표로 한다. \n",
    "\n",
    "Hyper-Parameter 조정을 하며 최적의 모델을 찾아낸다.\n",
    "\n",
    "1. Learning Rate\n",
    "1. Optimizer\n",
    "1. batch size\n",
    "1. Max Epochs\n",
    "1. 그외 모델 관련 Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 모델 만들기(TODO1)\n",
    "\n",
    "신경망 모델에는 다음과 같은 Hyper-Parameter들이 존재한다.\n",
    "\n",
    "1. Layer 갯수 : 최대 4개 이하 권장\n",
    "1. Node 갯수\n",
    "1. activation 함수 : sigmoid / relu\n",
    "\n",
    "적당한 값일 잘 선택하여 `curr_loss`를 0.05 밑으로만 내려가게 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "x = tf.placeholder(tf.float32, [None, features.shape[1]])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "output = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비용 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent(TODO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO 2\n",
    "learning_rate = None\n",
    "train = None\n",
    "batch_size = None\n",
    "MaxEpochs = None\n",
    "\n",
    "# Shuffling\n",
    "idx = np.arange(len(features))\n",
    "np.random.shuffle(idx)\n",
    "shuffled_features = features.values[idx]\n",
    "shuffled_labels = targets['cnt'].values[idx].reshape(-1,1)\n",
    "\n",
    "# SGD\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for epoch in range(MaxEpochs):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_features, shuffled_labels):\n",
    "        sess.run(train, feed_dict={x: x_batch, y:y_batch})\n",
    "    if epoch % 20 == 0:\n",
    "        curr_loss = sess.run(loss, feed_dict={x: shuffled_features, y:shuffled_labels})\n",
    "        print(epoch, curr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델의 예측값 정확도\n",
    "\n",
    "다음 그림과 비슷하게 나와야 한다.\n",
    "\n",
    "![](an_answer.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions= sess.run(output, feed_dict={x: test_features, y:test_targets['cnt'].values.reshape(-1,1)})\n",
    "predictions = predictions.T * std + mean\n",
    "\n",
    "ax.plot(predictions[0], label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
