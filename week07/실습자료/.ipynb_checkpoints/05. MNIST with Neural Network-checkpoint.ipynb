{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "for k in range(32):\n",
    "    img = mnist.train.images[k].reshape(28,28)\n",
    "    label = np.argmax(mnist.train.labels[k])\n",
    "    plt.subplot(4,8,1+k)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Min : {0}, Max : {1}\".format(mnist.train.images[0].min(), mnist.train.images[0].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Classification\n",
    "$$\n",
    "\\min_{W,b} \\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{10} -y_i^k \\log(\\hat{y_i^k})\n",
    "$$\n",
    "where $\\hat{y_i^k} = softmax((\\sigma(xW_1+b_1)) W_2 + b_2)_k$\n",
    "\n",
    "1. Forward Model : $\\hat{y_i^k} = softmax((\\sigma(xW_1+b_1)) W_2 + b_2)_k$\n",
    "1. Cross-Entropy : $\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{10} -y_i^k \\log(\\hat{y_i^k})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Model에 맞는 feature 정리\n",
    "For $i=1,2,\\cdots,m$,\n",
    "$$\n",
    "\\texttt{features}_i = [\\texttt{pixel}^i_1, \\texttt{pixel}^i_2,\\cdots, \\texttt{pixel}^i_{784}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.images.shape)\n",
    "features = mnist.train.images\n",
    "labels = mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Model TensorFlow로 정의하기\n",
    "\n",
    "- TODO1 : `features`를 `feed`할 placeholder(`x`)를 정의\n",
    "- TODO2 : Weight(`W`)와 bias(`b`)를 `Variable`로 초기값 0으로 정의\n",
    "- TODO3 & TODO4: Model(`model`) 정의\n",
    "- `num_hidden` : 20\n",
    "$$\n",
    "\\hat{y_i^k} = softmax((\\sigma(xW_1+b_1)) W_2 + b_2)_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# TODO2\n",
    "num_hidden = 20\n",
    "W_hidden = None\n",
    "b_hidden = tf.Variable(tf.random_normal([num_hidden]))\n",
    "\n",
    "W_out = None\n",
    "b_out = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# TODO3\n",
    "hidden = None\n",
    "# TODO4\n",
    "model = None\n",
    "\n",
    "print(x.shape)\n",
    "print(W_hidden.shape)\n",
    "print(b_hidden.shape)\n",
    "print(W_out.shape)\n",
    "print(b_out.shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Cross-Entropy 정의하기\n",
    "\n",
    "$$\n",
    "\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{10} -y_i^k \\log(\\hat{y_i^k})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-y * tf.log(model),1))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(model,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06. Stochastic Gradient Descent에 사용할 Hyper-Parameter 설정\n",
    "1. Optimizer : `AdamOptimizer`\n",
    "1. `batch_size` : `128`\n",
    "1. `lerning rate` : `0.01`\n",
    "1. `MaxEpoch` : `21`\n",
    "1. Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.01\n",
    "MaxEpochs = 21\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07. Stochastic Gradient Descent(SGD)\n",
    "- `mnist.train.next_batch()`사용하여 batch 생성하기\n",
    "- batch만 사용하여, train 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for epoch in range(MaxEpochs):\n",
    "    for step in range(len(mnist.train.images) // batch_size + 1):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train, feed_dict={x:batch_xs, y:batch_ys})\n",
    "        if step % 200 == 0:\n",
    "            curr_loss = sess.run(loss, feed_dict={x:features, y:labels})\n",
    "            curr_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "            print(epoch, step, curr_loss, curr_acc)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 9229\n",
    "test_image = mnist.test.images[test_id]\n",
    "test_label = mnist.test.labels[test_id]\n",
    "\n",
    "out = sess.run(model, feed_dict={x:[test_image]})\n",
    "print(\"Predicted Label : {0} (Real Label : {1})\".format(np.argmax(out), np.argmax(test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_image.reshape(28,28)\n",
    "label = np.argmax(test_label)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Predicted Label : {0} (Real Label : {1})\".format(np.argmax(out), np.argmax(test_label)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE\n",
    "이번엔 아래와 같은 loss function을 사용하여, SGD를 구현해보세요.\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^{10} |\\hat{y_i^k}-y_i^k|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mnist.train.images\n",
    "labels = mnist.train.labels\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "num_hidden = 20\n",
    "W_hidden = tf.Variable(tf.random_normal([784, num_hidden]))\n",
    "b_hidden = tf.Variable(tf.random_normal([num_hidden]))\n",
    "\n",
    "W_out = tf.Variable(tf.random_normal([num_hidden, 10]))\n",
    "b_out = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hidden = tf.nn.sigmoid(tf.matmul(x, W_hidden) + b_hidden)\n",
    "model = tf.nn.softmax(tf.matmul(hidden, W_out) + b_out)\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(model - y),1))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(model,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "MaxEpochs = 21\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for epoch in range(MaxEpochs):\n",
    "    # TODO 10\n",
    "    for step in range(len(mnist.train.images) // batch_size + 1):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # TODO 11\n",
    "        sess.run(train, feed_dict={x:batch_xs, y:batch_ys})\n",
    "        if step % 200 == 0:\n",
    "            curr_loss = sess.run(loss, feed_dict={x:features, y:labels})\n",
    "            curr_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "            print(epoch, step, curr_loss, curr_acc)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
