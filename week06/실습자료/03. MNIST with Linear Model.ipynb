{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "freq = Counter([np.argmax(label) for label in mnist.train.labels])\n",
    "for k in range(0,10):\n",
    "    print(\"label {0} : {1}개\".format(k, freq[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mnist.train.images[0].shape)\n",
    "print(mnist.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "for k in range(32):\n",
    "    img = mnist.train.images[k].reshape(28,28)\n",
    "    label = np.argmax(mnist.train.labels[k])\n",
    "    plt.subplot(4,8,1+k)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Min : {0}, Max : {1}\".format(mnist.train.images[0].min(), mnist.train.images[0].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Classification\n",
    "$$\n",
    "\\min_{W,b} \\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{10} -y_i^k \\log(\\hat{y_i^k})\n",
    "$$\n",
    "where $\\hat{y_i^k} = softmax(x_i W + b)_k$\n",
    "\n",
    "1. Forward Model : $\\hat{y_i^k} = softmax(x_i W + b)_k$\n",
    "1. Cross-Entropy : $\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{10} -y_i^k \\log(\\hat{y_i^k})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Model에 맞는 feature 정리\n",
    "For $i=1,2,\\cdots,m$,\n",
    "$$\n",
    "\\texttt{features}_i = [\\texttt{pixel}^i_1, \\texttt{pixel}^i_2,\\cdots, \\texttt{pixel}^i_{784}]\n",
    "$$\n",
    "\n",
    "- TODO1 : mnist.train.images가 어떤 구조로 되어 있는지 파악하고, 위의 feature에 맞게 정리가 필요하다면 정리하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1\n",
    "print(mnist.train.images.shape)\n",
    "features = mnist.train.images\n",
    "labels = mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Model TensorFlow로 정의하기\n",
    "\n",
    "- TODO2 : `features`를 `feed`할 placeholder(`x`)를 정의\n",
    "- TODO3 : Weight(`W`)와 bias(`b`)를 `Variable`로 초기값 0으로 정의\n",
    "- TODO4 : Model(`model`) 정의\n",
    "\n",
    "$$\n",
    "\\hat{y_i^k} = softmax(x_i W + b)_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2\n",
    "x = tf.placeholder(tf.float32, shape=(None, None))\n",
    "\n",
    "# TODO3\n",
    "W = tf.Variable(tf.zeros([None, None]))\n",
    "b = tf.Variable(tf.zeros([None]))\n",
    "\n",
    "# TODO4\n",
    "model = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "print(x.shape)\n",
    "print(W.shape)\n",
    "print(b.shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Cross-Entropy 정의하기\n",
    "\n",
    "- TODO5 : `labels`를 `feed`할 `placeholder`(`y`) 정의하기\n",
    "- TODO6 : Corss-Entropy(`loss`) 정의하기 : `tf.\n",
    "\n",
    "$$\n",
    "\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{10} -y_i^k \\log(\\hat{y_i^k})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO6\n",
    "y = tf.placeholder(tf.flo32, shape=(None, 10))\n",
    "\n",
    "# TODO7\n",
    "loss = tf.reduce_mean(tf.reduce_sum(None,1))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. Gradient Descent에 사용할 Hyper-Parameter 설정(TODO 7) \n",
    "1. Optimizer : `AdamOptimizer`\n",
    "1. `lerning rate` : `0.01`\n",
    "1. `MaxEpoch` : `51`\n",
    "1. Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO7\n",
    "lr = 0.01\n",
    "MaxEpochs = 51\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(model,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. Gradient Descent\n",
    "\n",
    "- TODO8 : batch 사용하지 않고 모든 features와 labels을 다 feed 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for epoch in range(MaxEpochs):\n",
    "    # TODO 8\n",
    "    sess.run(train, feed_dict={x:features, y:labels})\n",
    "    if epoch % 5 == 0:\n",
    "        curr_W, curr_b, curr_loss = sess.run([W, b, loss], feed_dict={x:features, y:labels})\n",
    "        curr_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "        print(epoch, curr_loss, curr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06. Stochastic Gradient Descent에 사용할 Hyper-Parameter 설정(TODO 9) \n",
    "1. Optimizer : `AdamOptimizer`\n",
    "1. `batch_size` : `128`\n",
    "1. `lerning rate` : `0.01`\n",
    "1. `MaxEpoch` : `5`\n",
    "1. Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO9\n",
    "batch_size = None\n",
    "lr = None\n",
    "MaxEpochs = None\n",
    "optimizer = None\n",
    "train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07. Stochastic Gradient Descent(SGD)\n",
    "- TODO10 : `mnist.train.next_batch()`사용하여 batch 생성하기\n",
    "- TODO11 : batch만 사용하여, train 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for epoch in range(MaxEpochs):\n",
    "    # TODO 10\n",
    "    for step in range(len(mnist.train.images) // batch_size + 1):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(None)\n",
    "        # TODO 11\n",
    "        None\n",
    "        if step % 50 == 0:\n",
    "            curr_W, curr_b, curr_loss = sess.run([W, b, loss], feed_dict={x:features, y:labels})\n",
    "            curr_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "            print(epoch, step, curr_loss, curr_acc)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 9229\n",
    "test_image = mnist.test.images[test_id]\n",
    "test_label = mnist.test.labels[test_id]\n",
    "\n",
    "out = sess.run(model, feed_dict={x:[test_image]})\n",
    "print(\"Predicted Label : {0} (Real Label : {1})\".format(np.argmax(out), np.argmax(test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_image.reshape(28,28)\n",
    "label = np.argmax(test_label)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Predicted Label : {0} (Real Label : {1})\".format(np.argmax(out), np.argmax(test_label)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE\n",
    "이번엔 아래와 같은 loss function을 사용하여, SGD를 구현해보세요.\n",
    "\n",
    "$$\n",
    "E = \\sum_{i=1}^m \\sum_{k=1}^{10} |\\hat{y_i^k}-y_i^k|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mnist.train.images\n",
    "labels = mnist.train.labels\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "model = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# TODO12\n",
    "loss = None\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(model,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "MaxEpochs = 5\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for epoch in range(MaxEpochs):\n",
    "    for step in range(len(mnist.train.images) // batch_size + 1):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train, feed_dict={x:batch_xs, y:batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            curr_W, curr_b, curr_loss = sess.run([W, b, loss], feed_dict={x:features, y:labels})\n",
    "            curr_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "            print(epoch, step, curr_loss, curr_acc)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
