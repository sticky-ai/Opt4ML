{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Reression Problem\n",
    "데이터$(x_i,y_i)$ for $i=1,2,\\cdots, N$가 주어져 있을 때, 다음 문제를 푸시오.\n",
    "\\begin{equation}\n",
    "\\min_w loss(w,x,y)\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "loss(w,x,y) = \\frac{1}{N}\\sum_{i=1}^N |w_0x_i + w_1 - y_i|^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "f = lambda x: 0.3 * x + 5.0 # Target function\n",
    "x_train = np.linspace(-1, 1, N)\n",
    "np.random.seed(313)\n",
    "y_train = f(x_train) + 0.2 * np.random.rand(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train,y_train, 'o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리뷰 Gradient Descent Method\n",
    "현재까지 배운 수치최적화 방법은 아래와 같이 3가지가 있습니다.\n",
    "1. Gradient Descent Method(=Steepest Descent Method)\n",
    "1. Newton Method\n",
    "1. BFGS Method : Quasi-Newton(Hessian 계산을 하지 않습니다.)\n",
    "\n",
    "Newton method와 BFGS method는 convex문제에서 굉장히 빠른 장점을 갖고 있지만, 안정성이 Gradient Descent에 비해서 낮습니다. 실무에서는 머신러닝/딥러닝을 고려하기때문에 안정성이 더 높은 Gradient Descent를 자주 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "1. loss function을 정의힌다.\n",
    "1. gradient function을 정의힌다.\n",
    "1. Parameter를 Tunning한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "loss(w) = \\frac{1}{N}\\sum_{i=1}^N |w_0x_i + w_1 - y_i|^2\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\nabla loss(w) = \\frac{2}{N}\\sum_{i=1}^N (w_0x_i + w_1 - y_i)\n",
    "\\begin{bmatrix}\n",
    "x_i\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(w, x_list, y_list):\n",
    "    N = len(x_list)\n",
    "    val = 0.0\n",
    "    for i in range(N):\n",
    "        val += (w[0] * x_list[i] + w[1] - y_list[i])**2 / N\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_loss(w, x_list, y_list):\n",
    "    dim = len(w)\n",
    "    N = len(x_list)\n",
    "    val = np.array([0.0, 0.0])\n",
    "    for i in range(N):\n",
    "        er = w[0] * x_list[i] + w[1] - y_list[i]\n",
    "        val += 2.0 * er * np.array([x_list[i], 1.0]) / N\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W0 = np.linspace(-10, 10, 101)\n",
    "W1 = np.linspace(-10, 10, 101)\n",
    "W0, W1 = np.meshgrid(W0,W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i,j], W1[i,j]])\n",
    "        LOSSW[i,j] = loss(wij, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import gradient_descent\n",
    "w0 = np.array([-5.0, -5.0])\n",
    "w_gd, path_gd = gradient_descent(grad_loss, x_train, y_train, w0, learning_rate=0.1, MaxIter=500)\n",
    "print(w_gd, loss(w_gd, x_train, y_train))\n",
    "paths = path_gd\n",
    "paths = np.array(np.matrix(paths).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w_gd[0] * x_train + w_gd[1]\n",
    "plt.plot(x_train,y_train, 'o')\n",
    "plt.plot(x_train, y_pred, '-r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contour(W0, W1, LOSSW, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW.flatten()),30))\n",
    "ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch\n",
    "머신러닝/딥러닝에서는 데이터가 매우 많습니다. 이런 경우에는 `loss()`을 계산시 2가지 문제가 발생합니다.\n",
    "1. 데이터로드 할 메모리가 부족\n",
    "1. 계산 시간 오래 걸림\n",
    "\n",
    "그래서, 데이터를 나눠서 계산하는 방법을 선택합니다. 이번 섹션에서는 `generate_batches()`를 사용하여 큰 data를 나누어 mini-batch들로 만드는 함수를 소개합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import generate_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_batches = generate_batches(5, x_train, y_train)\n",
    "for x_batch, y_batch in out_batches:\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Method의 핵심 아이디어\n",
    "1. `x_train`와 `y_train`이 너무 많아서 계산이 오래 걸린다. 혹은 메모리가 부족하다.\n",
    "1. `x_train`와 `y_train`의 일부만으로 gradient 계산하자.\n",
    "1. RANDOM 하게 섞으면 일부분만으로도 충분히 gradient를 잘 추정할 수 있을 것이다.\n",
    "\n",
    "__STOCHASTIC = RANDOM__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Method의 장/단점\n",
    "- 장점\n",
    "    1. 메모리를 아낄 수 있다.\n",
    "    1. 계산이 빠르다.\n",
    "    1. Local Minimum에서 벗어 날 가능성이 있다.\n",
    "- 단점\n",
    "    1. Gradient 값이 정확하지 않다.\n",
    "    1. 그래서 수렴속도가 느리다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 포인트 ( TODO 1 ~ TODO 4 )\n",
    "1. `np.random.suffle()`을 사용하여 `x_train`과 `y_train`을 섞습니다.\n",
    "    - Hint 1\n",
    "```python\n",
    "shuffled_id = np.arange(0, N)\n",
    "np.random.shuffle(shuffled_id)\n",
    "```\n",
    "    - Hint 2\n",
    "```python\n",
    "ids = np.arange(0,4)\n",
    "print(ids)\n",
    "np.random.shuffle(ids)\n",
    "print(ids)\n",
    "```\n",
    "```\n",
    "ids = [0, 1, 2, 3]\n",
    "ids = [2, 0, 3, 1]\n",
    "```\n",
    "    - Hint 3\n",
    "```python\n",
    "x = np.array([10, 20, 30, 40])\n",
    "ids = np.array([2, 0, 3, 1])\n",
    "print(x[ids])\n",
    "```\n",
    "```\n",
    "x = [30, 10, 40, 20]\n",
    "```\n",
    "1. `generate_batches()`를 사용하여 여러개의 batch로 나눕니다. : `batch_size = 5`\n",
    "```python\n",
    "for x_batch, y_batch in generate_batches(batch_size, x_train, y_train):\n",
    "```\n",
    "1. 각 나눈 batch 들로 Gradient를 계산하고\n",
    "```python\n",
    "grad = grad_loss(w0, x_batch, y_batch)\n",
    "```\n",
    "1. Gradient Decent Method를 적용\n",
    "```python\n",
    "w1 = w0 - learning_rate * grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 20, 30, 40])\n",
    "ids = np.array([2, 0, 3, 1])\n",
    "print(ids)\n",
    "print(x[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "path_sgd = []\n",
    "cost_history_sgd = []\n",
    "w0 = np.array([-5.0, -5.0])\n",
    "path_sgd.append(w0)\n",
    "learning_rate = 0.1\n",
    "MaxIter = 100\n",
    "\n",
    "shuffled_id = np.arange(0, N)\n",
    "#TODO1\n",
    "None\n",
    "x_train = None\n",
    "y_train = None\n",
    "for i in range(MaxIter):\n",
    "    for x_batch, y_batch in generate_batches(None, None, None):#TODO2\n",
    "        grad = grad_loss(None, None, None)#TODO3\n",
    "        w1 = None#TODO4\n",
    "        w0 = w1\n",
    "        path_sgd.append(w1)\n",
    "        cost_history_sgd.append(loss(w0, x_batch, y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구현 후 아래 코드를 실행하면 \n",
    "다음과 같은 그림을 얻게 됩니다.\n",
    "![SGD.png](SGD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = path_sgd\n",
    "paths = np.array(np.matrix(paths).T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contour(W0, W1, LOSSW, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW.flatten()),20))\n",
    "ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 비교\n",
    "![GD.png](GD.png)\n",
    "![SGD.png](SGD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_history_sgd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_history_sgd[30:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
