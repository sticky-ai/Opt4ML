{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptvie scheme *직접* 적용해보기\n",
    "`TensorFlow`에서 사용하는 `Optimizer`들은 기본적으로 Gradient Descent(=Steepest Descent) Method 기반에서 `learning_rate`과 search direction의 minor variated version 입니다.\n",
    "\n",
    "예를 들면, \n",
    "1. [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer)\n",
    "1. [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)\n",
    "1. [`tf.train.AdadeltaOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer)\n",
    "1. [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "\n",
    "이번 실습에서는 위와 같은 adaptive scheme의 가장 기본적으로 접근법 중 하나로 `learning_rate` 만을 조정하는 법을 실습해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습에서 사용할 예제\n",
    "주어진 데이터 $(x_i, y_i)$ for $i=1,2, \\cdots, 100$에 대하여 아래 loss function을 최소화하는 $a$와 $b$를 구하시오.\n",
    "\\begin{equation}\n",
    "\\min_{w_0,w_1} \\frac{1}{100}\\sum_{i=1}^{100}|w_0x_i+w_1 - y_i|^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "f = lambda x: 0.3 * x + 5.0 # Target function\n",
    "x_train = np.linspace(-1, 1, N)\n",
    "np.random.seed(313)\n",
    "y_train = f(x_train) + 0.2 * np.random.rand(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train,y_train, 'o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function의 input parameter 추가\n",
    "1. 지금까지는 `loss(w)`로 알 수 있듯이, `w`만 input으로 받고, `data`는 글로벌 변수로 사용했습니다.\n",
    "1. 관찰해보면 `loss(w)`은 `data`(`x_train`, `y_train`)에 따라서  변하게 됩니다.\n",
    "1. 그러므로 `loss(w, x_set, y_set)`이 더 정확한 loss function의 표현입니다.\n",
    "1. 추후 Stochastic Gradient Descent(SGD)에서도 이러한 loss function을 사용합니다.\n",
    "1. 또한 `TensorFlow`에서도 이러한 방식으로 loss function이 표현됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(w, x_list, y_list):\n",
    "    N = len(x_list)\n",
    "    val = 0.0\n",
    "    for i in range(N):\n",
    "        val += (w[0] * x_list[i] + w[1] - y_list[i])**2 / N\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_loss(w, x_list, y_list):\n",
    "    dim = len(w)\n",
    "    N = len(x_list)\n",
    "    val = np.array([0.0, 0.0])\n",
    "    for i in range(N):\n",
    "        er = w[0] * x_list[i] + w[1] - y_list[i]\n",
    "        val += 2.0 * er * np.array([x_list[i], 1.0]) / N\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate이 큰 경우\n",
    "`learning_rate=1.5`로 설정하면, loss function이 계속 커집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0 = np.array([-5.0, -5.0])\n",
    "learning_rate = 1.5\n",
    "MaxIter = 10\n",
    "for i in range(MaxIter):\n",
    "    loss0 = loss(w0, x_train, y_train)\n",
    "    grad = grad_loss(w0, x_train, y_train)\n",
    "    w1 = w0 - learning_rate * grad\n",
    "    print(i, w0, loss0)\n",
    "    w0 = w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate이 작은 경우\n",
    "`learning_rate=0.0001`로 설정하면, loss function이 계속 작아지지만, 너무 느립니다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0 = np.array([-5.0, -5.0])\n",
    "learning_rate = 0.0001\n",
    "MaxIter = 10\n",
    "for i in range(MaxIter):\n",
    "    loss0 = loss(w0, x_train, y_train)\n",
    "    grad = grad_loss(w0, x_train, y_train)\n",
    "    w1 = w0 - learning_rate * grad\n",
    "    print(i, w0, loss0)\n",
    "    w0 = w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1 : 자동으로 learning_rate을 조정하는 아이디어는 없을까?\n",
    "매번 `learning_rate`을 찾는 것도 피곤한데, 자동으로 찾게 할 방법을 찾아보려합니다. 이번 실습에서는 아래와 같은 방식으로 코드를 작성해봅니다.\n",
    "\n",
    "1. `loss(w1,x_train,y_train)`가 `loss(w0,x_train,y_train)`보다 커질때마다, `learning_rate`을 2배씩 작게 하고\n",
    "1. 다시 next position(`w1`)을 업데이트 하고 `loss(w1,x_train,y_train)`을 계산해서 `loss(w0,x_train,y_train)` 값과 비교해봅니다.\n",
    "1. `loss(w1,x_train,y_train) < loss(w0,x_train,y_train)`이 참값이 될때까지, `learning_rate`를 2배씩 줄입니다.\n",
    "\n",
    "__`#TODO`안을 채우시면 됩니다.__\n",
    "\n",
    "__HINT:__\n",
    "```python\n",
    "if loss(w1) > loss(w0): # loss(w1) 값이 loss(w0)보다 크다면\n",
    "    #learning_rate을 줄입니다.\n",
    "else: # loss(w1) 값이 loss(w0)보다 작다면\n",
    "    #업데이트 합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0 = np.array([-5.0, -5.0])\n",
    "learning_rate = 10\n",
    "MaxIter = 10\n",
    "for i in range(MaxIter):\n",
    "    # TODO 1\n",
    "    loss0 = loss(None, None, None)\n",
    "    grad = grad_loss(None, None, None)\n",
    "    w1 = w0 - learning_rate * grad\n",
    "    loss1 = loss(None, None, None)\n",
    "    if None:\n",
    "        learning_rate = None\n",
    "    else:\n",
    "        w0 = None\n",
    "    print(i, w0, loss0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
