{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD를 사용하여 2차 함수 모델 fiiting하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "f = lambda x: x**2 + 1.0/3.0 * x + 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.linspace(-20, 60, 50)\n",
    "fx = f(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(313)\n",
    "y_train = fx + 500 * np.random.rand(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train,y_train, 'o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "loss(w) = \\frac{1}{N}\\sum_{i=1}^N |w_0 x_i^2 + w_1x_i + w_2 - y_i|^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, x_set, y_set):\n",
    "    num_data = len(x_set)\n",
    "    val = 0.0\n",
    "    for i in range(num_data):\n",
    "        val += 1.0 * (w[0] * x_set[i]**2 + w[1] * x_set[i] + w[2] - y_set[i])**2\n",
    "    return val / num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent 사용하기\n",
    "1. Define gradient\n",
    "1. Tune Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "loss(w) = \\frac{1}{N}\\sum_{i=1}^N |w_0 x_i^2 + w_1x_i + w_2 - y_i|^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla loss(w) =\n",
    "\\frac{2}{N}\\sum_{i=1}^N\n",
    "(w_0 x_i^2 + w_1x_i + w_2 - y_i)\n",
    "\\begin{bmatrix}\n",
    "x_i^2\\\\\n",
    "x_i\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(w, x_set, y_set):\n",
    "    num_data = len(x_set)\n",
    "    val = np.zeros(len(w))\n",
    "    for i in range(num_data):\n",
    "        er = w[0] * x_set[i]**2 + w[1] * x_set[i] + w[2] - y_set[i]\n",
    "        val += 2.0 * er * np.array([x_set[i]**2, x_set[i], 1.0])\n",
    "    return val / num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent_3d(loss, grad_func, w0, x_set, y_set, learning_rate=0.01, MaxIter=10):\n",
    "    for i in range(MaxIter):\n",
    "        w1 = w0 -learning_rate * grad_func(w0, x_set, y_set)\n",
    "        w0 = w1\n",
    "    return w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0, 1.0])\n",
    "w_gd = steepest_descent_3d(loss, grad_loss, w0, x_train, y_train, \\\n",
    "                           learning_rate=2E-7, MaxIter=2500)\n",
    "print(w_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w_gd[0] * x_train ** 2 + w_gd[1] * x_train + w_gd[2]\n",
    "plt.plot(x_train,y_train, 'o')\n",
    "plt.plot(x_train,y_pred, 'r-')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Normalization 하기\n",
    "1. min/max normalization\n",
    "```python\n",
    "scaled_x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "```\n",
    "1. mean/variance normailzation\n",
    "```python\n",
    "scaled_x = (x - np.mean(x)) / np.sqrt(np.var(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. min/max normalization\n",
    "```python\n",
    "scaled_x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x_train1 = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
    "print(x_train)\n",
    "print(scaled_x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0, 1.0])\n",
    "w_gd_sc1 = steepest_descent_3d(loss, grad_loss, w0, scaled_x_train1, y_train, \\\n",
    "                           learning_rate=.2, MaxIter=2500)\n",
    "print(w_gd_sc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w_gd_sc1[0] * scaled_x_train1 ** 2 + w_gd_sc1[1] * scaled_x_train1 + w_gd_sc1[2]\n",
    "plt.plot(scaled_x_train1, y_train, 'o')\n",
    "plt.plot(scaled_x_train1, y_pred, 'r-')\n",
    "plt.grid()\n",
    "plt.xlabel('scaled x')\n",
    "plt.xlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. mean/variance normalization\n",
    "```python\n",
    "scaled_x = (x - np.mean(x)) / np.sqrt(np.var(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x_train2 = (x_train - np.mean(x_train)) / np.sqrt(np.var(x_train))\n",
    "print(x_train)\n",
    "print(scaled_x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0, 1.0])\n",
    "w_gd_sc2 = steepest_descent_3d(loss, grad_loss, w0, scaled_x_train2, y_train, \\\n",
    "                           learning_rate=.2, MaxIter=2500)\n",
    "print(w_gd_sc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w_gd_sc2[0] * scaled_x_train2 ** 2 + w_gd_sc2[1] * scaled_x_train2 + w_gd_sc2[2]\n",
    "plt.plot(scaled_x_train2, y_train, 'o')\n",
    "plt.plot(scaled_x_train2, y_pred, 'r-')\n",
    "plt.grid()\n",
    "plt.xlabel('scaled x')\n",
    "plt.xlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stochastic Gradient Descent\n",
    "1. `np.random.shuffle()`을 사용하여 `x_train`을 섞는다.\n",
    "1. `generate_batches()`를 사용하여 batch들을 만든다.\n",
    "1. Stochastic Gradient Method를 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    out_batches = []\n",
    "\n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        out_batches.append(batch)\n",
    "\n",
    "    return out_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Shuffle하기\n",
    "```python\n",
    "np.random.shuffle(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(len(x_train))\n",
    "print(a)\n",
    "print(x_train[a])\n",
    "np.random.shuffle(a)\n",
    "print(a)\n",
    "print(x_train[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Batch 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "for x_train_batch, y_train_batch in generate_batches(batch_size, scaled_x_train1, y_train):\n",
    "    print('x_batch = {0}'.format(x_train_batch))\n",
    "    print('y_batch = {0}'.format(y_train_batch))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Stochastic Gradient Descent 적용\n",
    "1. min/max normalization을 적용\n",
    "1. `np.random.shuffle()` 이용하여 데이터 골고루 섞기\n",
    "1. 다음과 같이 Parameter를 설정\n",
    "    1. `batch_size=10`\n",
    "    1. `learning_rate=0.2`\n",
    "    1. `w0=np.array([1.0, 1.0, 1.0])`\n",
    "    1. `MaxEpochs = 2500`\n",
    "1. 아래 for loop 안에 SGD를 구현하시면 됩니다.\n",
    "    ```python\n",
    "    for epoch in range(MaxEpochs):\n",
    "        for x_batch, y_batch in generate_batches(_, _, _):\n",
    "            grad = grad_loss(w0, x_batch, y_batch)\n",
    "            # do gradient descent with x_batch and y_batch\n",
    "    ```\n",
    "1. SGD 구현을 올바르게하고, 위의 parameter로 설정하셨다면, 다음과 비슷한 그림이 나와야합니다.\n",
    "![week3_project_result.png](week3_project_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "scaled_x_train = None\n",
    "# TODO 2\n",
    "idx = np.arange(len(x_train))\n",
    "np.random.shuffle(None)\n",
    "\n",
    "sh_scaled_x_train = scaled_x_train[None]\n",
    "sh_y_train = y_train[None]\n",
    "# TODO 3\n",
    "batch_size = None\n",
    "MaxEpochs = None\n",
    "learning_rate = None\n",
    "w0 = np.array([1,1,1])\n",
    "for epoch in range(MaxEpochs):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, sh_scaled_x_train, sh_y_train):\n",
    "        # TODO 4\n",
    "        grad = grad_loss(None, None, None)\n",
    "        w1 = None\n",
    "        w0 = w1\n",
    "w_sgd = w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5(Just run, Don't modify below)\n",
    "y_pred = w_sgd[0] * scaled_x_train ** 2 + w_sgd[1] * scaled_x_train + w_sgd[2]\n",
    "plt.plot(scaled_x_train, y_train, 'o')\n",
    "plt.plot(scaled_x_train, y_pred, 'r-')\n",
    "plt.grid()\n",
    "plt.xlabel('scaled x')\n",
    "plt.xlabel('y')\n",
    "plt.title('SGD : loss = {0} '.format(loss(w_sgd, scaled_x_train, y_train)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
