{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Learning Rate\n",
    "`helper.py`에는 아래와 같은 함수들이 들어있습니다.\n",
    "\n",
    "1. Gradient\n",
    "1. Newton\n",
    "1. BFGS\n",
    "1. Nelder-Mead\n",
    "1. Momentum\n",
    "1. Nesterov Momentum\n",
    "1. Adagrad\n",
    "1. RMSprop\n",
    "1. Adam\n",
    "\n",
    "이번 실습에서는 Momentum Algorithm, Nesterov Momentum과 RmsProp에 빈칸을 채워 동작하도록 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(320)\n",
    "x_train = np.linspace(-1, 1, 51)\n",
    "f = lambda x: 0.5 * x + 1.0\n",
    "y_train = f(x_train) + 0.4 * np.random.rand(len(x_train))\n",
    "\n",
    "plt.plot(x_train, y_train, 'o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, x_list, y_list):\n",
    "    N = len(x_list)\n",
    "    val = 0.0\n",
    "    for i in range(N):\n",
    "        val += (w[0] * x_list[i] + w[1] - y_list[i])**2 / N\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(w, x_list, y_list):\n",
    "    N = len(x_list)\n",
    "    val = np.array([0.0, 0.0])\n",
    "    for i in range(N):\n",
    "        er = w[0] * x_list[i] + w[1] - y_list[i]\n",
    "        val += 2.0 * er * np.array([x_list[i], 1.0]) / N\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Algorithm\n",
    "Let $v_0 = \\vec{0}$, $\\alpha = 0.9$, and $\\epsilon=0.01$. For $k=1,2,3,\\cdots,$, \n",
    "\\begin{align}\n",
    "v_{k+1} & = \\alpha v_{k} - \\epsilon \\nabla f(x_k)\\\\\n",
    "x_{k+1} &= x_k + v_{k} \n",
    "\\end{align}\n",
    "\n",
    "1. $\\nabla f(x_k)$ 계산하기\n",
    "1. $v_{k+1} = \\alpha v_{k} - \\epsilon \\nabla f(x_k)$ 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_method(grad_func, x_set, y_set, w0,\n",
    "                    learning_rate=0.01, alpha=0.9, MaxIter=10):\n",
    "    epsilon = learning_rate\n",
    "    velocity = np.zeros_like(w0)\n",
    "    for i in range(MaxIter):\n",
    "        # TODO1\n",
    "        grad = grad_loss(w0, x_set, y_set)\n",
    "        # TODO2\n",
    "        velocity = None - epsilon * grad\n",
    "        w1 = w0 + velocity\n",
    "        w0 = w1\n",
    "    return w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0])\n",
    "w_mm = momentum_method(grad_loss, x_train, y_train, w0, learning_rate=0.01, MaxIter=500)\n",
    "print(w_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'o')\n",
    "plt.plot(x_train, w_mm[0] * x_train + w_mm[1] , '-r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum Algorithm\n",
    "Let $v_0 = \\vec{0}$, $\\alpha = 0.9$, and $\\epsilon=0.01$. For $k=1,2,3,\\cdots,$, \n",
    "\\begin{align}\n",
    "v_{k+1} & = \\alpha v_{k} - \\epsilon \\nabla f(x_k+\\alpha v_{k})\\\\\n",
    "x_{k+1} &= x_k + v_{k+1} \n",
    "\\end{align}\n",
    "\n",
    "1. $\\nabla f(x_k+\\alpha v_{k})$ 계산하기\n",
    "1. $v_{k+1} = \\alpha v_{k} - \\epsilon \\nabla f(x_k+\\alpha v_{k})$ 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_method(grad_func, x_set, y_set, w0,\n",
    "                    learning_rate=0.01, alpha=0.9, MaxIter=10):\n",
    "    epsilon = learning_rate\n",
    "    velocity = np.zeros_like(w0)\n",
    "    for i in range(MaxIter):\n",
    "        # TODO1\n",
    "        grad = None\n",
    "        # TODO2\n",
    "        velocity = None\n",
    "        w1 = w0 + velocity\n",
    "        w0 = w1\n",
    "    return w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0])\n",
    "w_nmm = nesterov_method(grad_loss, x_train, y_train, w0, learning_rate=0.01, MaxIter=500)\n",
    "print(w_nmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'o')\n",
    "plt.plot(x_train, w_nmm[0] * x_train + w_nmm[1] , '-r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "Let $r_0 = \\vec{0}$, $\\delta = 10^{-7}$, and $\\epsilon=0.01$. For $k=1,2,3,\\cdots,$, \n",
    "\\begin{align}\n",
    "r_{k+1} & =  r_{k} + \\nabla f(x_k) \\odot   \\nabla f(x_k)\\\\\n",
    "\\varDelta x_{k}  & = - \\frac{\\epsilon}{\\delta + \\sqrt{r_{k+1}}}\\odot \\nabla f(x_k) \\\\\n",
    "x_{k+1} &= x_k + \\varDelta x_{k} \n",
    "\\end{align}\n",
    "\n",
    "1. $\\nabla f(x_k)$ 계산하기\n",
    "1. $\\varDelta x_{k}  = - \\frac{\\epsilon}{\\delta + \\sqrt{r_{k+1}}}\\odot \\nabla f(x_k)$ 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad_method(grad_func, x_set, y_set, w0,\n",
    "                    learning_rate=1, delta=1E-7, MaxIter=10):\n",
    "    epsilon = learning_rate\n",
    "    r = np.zeros_like(w0)\n",
    "    for i in range(MaxIter):\n",
    "        # TODO1\n",
    "        grad = grad_loss(w0, x_set, y_set)\n",
    "        r = r  + grad * grad\n",
    "        # TODO2\n",
    "        delw = - epsilon / (delta + np.sqrt(r)) * grad\n",
    "        w1 = w0 + delw\n",
    "        w0 = w1\n",
    "    return w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0])\n",
    "w_adag = adagrad_method(grad_loss, x_train, y_train, w0, MaxIter=500)\n",
    "print(w_adag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'o')\n",
    "plt.plot(x_train, w_adag[0] * x_train + w_adag[1] , '-r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RMSProp\n",
    "\n",
    "Let $r_0 = \\vec{0}$, $\\delta = 10^{-7}$, and $\\epsilon=0.01$. For $k=1,2,3,\\cdots,$, \n",
    "\\begin{align}\n",
    "r_{k+1} & =  \\rho r_{k} + (1-\\rho)\\nabla f(x_k) \\odot   \\nabla f(x_k)\\\\\n",
    "\\varDelta x_{k}  & = - \\frac{\\epsilon}{\\sqrt{\\delta + r_{k+1}}}\\odot \\nabla f(x_k) \\\\\n",
    "x_{k+1} &= x_k + \\varDelta x_{k} \n",
    "\\end{align}\n",
    "\n",
    "1. $r_{k+1}  =  \\rho r_{k} + (1-\\rho)\\nabla f(x_k) \\odot   \\nabla f(x_k)$ 계산하기\n",
    "1. $\\varDelta x_{k}   = - \\frac{\\epsilon}{\\sqrt{\\delta + r_{k+1}}}\\odot \\nabla f(x_k)$ 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_method(grad_func, x_set, y_set, w0,\n",
    "                    learning_rate=0.01, delta=1E-6, rho=0.9, MaxIter=10):\n",
    "    epsilon = learning_rate\n",
    "    r = np.zeros_like(w0)\n",
    "    for i in range(MaxIter):\n",
    "        grad = grad_func(w0, x_set, y_set)\n",
    "        # TODO1\n",
    "        r = None\n",
    "        # TODO2\n",
    "        delw = None\n",
    "        w1 = w0 + delw\n",
    "        w0 = w1\n",
    "    return w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0])\n",
    "w_adag = adagrad_method(grad_loss, x_train, y_train, w0, MaxIter=500)\n",
    "print(w_adag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'o')\n",
    "plt.plot(x_train, w_adag[0] * x_train + w_adag[1] , '-r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
