{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Using cached https://files.pythonhosted.org/packages/86/02/e6b11020a9c37d25b4767a1d0af5835629f6e75d6f51553ad07a4c73dc31/inflect-2.1.0-py2.py3-none-any.whl\n",
      "\u001b[31mmkl-random 1.0.1 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.15.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-2.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "from pprint import pprint\n",
    "import inflect\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "for k in range(32):\n",
    "    img = mnist.train.images[k].reshape(28,28)\n",
    "    label = np.argmax(mnist.train.labels[k])\n",
    "    plt.subplot(4,8,1+k)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model with various Hyper-Parameters\n",
    "\n",
    "여러가지 Hyper-Parameter의 조합을 실험 후, 가장 좋은 결과를 찾아내는 프로젝트입니다.\n",
    "\n",
    "- `dictionary` 형태로 `params_grid`에 추가 하며 수정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid =[{'hidden_layer_info': [800, 400, 200], \n",
    "              'dropout_probs': [0.15, 0.15, 0.15], \n",
    "              'lr' : 0.001,\n",
    "              'batch_size' : 64, \n",
    "              'MaxEpochs' : 100,\n",
    "              'optimizer' : tf.train.AdamOptimizer,\n",
    "              'activation' : tf.nn.relu},\n",
    "              {'hidden_layer_info': [512, 256, 128], \n",
    "              'dropout_probs': [0.2, 0.2, 0.2], \n",
    "              'lr' : 0.001,\n",
    "              'batch_size' : 128, \n",
    "              'MaxEpochs' : 1,\n",
    "              'optimizer' : tf.train.GradientDescentOptimizer,\n",
    "              'activation' : tf.nn.relu},\n",
    "              {'hidden_layer_info': [30], \n",
    "              'dropout_probs': [0.0], \n",
    "              'lr' : 0.01,\n",
    "              'batch_size' : 32, \n",
    "              'MaxEpochs' : 3,\n",
    "              'optimizer' : tf.train.AdamOptimizer,\n",
    "              'activation' : tf.nn.sigmoid},\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network + Dropout\n",
    "\n",
    "- `prob = 0` : No dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logit(x, params, training=True):\n",
    "    activation_fn = params['activation']\n",
    "    hidden_layer = x\n",
    "    for hidden_nodes, prob in zip(params['hidden_layer_info'], params['dropout_probs']):\n",
    "        hidden_layer = tf.layers.dense(hidden_layer, hidden_nodes, activation=activation_fn)\n",
    "        hidden_layer = tf.layers.dropout(hidden_layer, rate=prob, training=training)\n",
    "    logits = tf.layers.dense(hidden_layer, 10, activation=None)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(y, logit, params):\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logit)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(params):\n",
    "    lr = params['lr']\n",
    "    optimizer = params['optimizer']\n",
    "    return optimizer(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 폴더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 Hyper-Parameter 실험해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "p = inflect.engine()\n",
    "for param_id, params in enumerate(params_grid, 1):\n",
    "    nth_string = p.ordinal(param_id)\n",
    "    print(\"============={0} Hyper-Parameter==============\".format(nth_string))\n",
    "    pprint(params)\n",
    "    print(\"============={0} Hyper-Parameter==============\".format(nth_string))\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    isTraining = tf.placeholder(tf.bool)\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    chosen_logits = build_logit(x, params, training=isTraining)\n",
    "    loss = build_loss(y, chosen_logits, params)\n",
    "    train = build_optimizer(params).minimize(loss)\n",
    "    \n",
    "    # saver\n",
    "    saver = tf.train.Saver(max_to_keep=25)\n",
    "    # For testing accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(chosen_logits,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    for epoch in range(params['MaxEpochs']):\n",
    "        train_loss = 0\n",
    "        for step in range(len(mnist.train.images) // batch_size + 1):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            batch_loss,_ = sess.run([loss, train], feed_dict={x:batch_xs, y:batch_ys, isTraining: True})\n",
    "            train_loss += batch_loss * len(batch_xs)\n",
    "            if step % 200 == 0:\n",
    "                curr_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels, isTraining: False})\n",
    "                print(epoch, step, curr_acc)\n",
    "                if not os.path.exists(\"./checkpoints/{0}\".format(nth_string)):\n",
    "                    os.mkdir(\"./checkpoints/{0}\".format(nth_string))\n",
    "                saver.save(sess, \"./checkpoints/{0}/model_epoch{1}_step_{2}.ckpt\".format(nth_string,epoch, step))\n",
    "        curr_acc, test_loss = sess.run([accuracy, loss], feed_dict={x:mnist.test.images, y:mnist.test.labels, isTraining: False})\n",
    "        train_loss /= len(mnist.train.images)\n",
    "        print(\"====>\", epoch, train_loss, test_loss, curr_acc)\n",
    "        print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_and_test(restore_id, epoch, step, params_grid):\n",
    "    tf.reset_default_graph()\n",
    "    param_id, params = restore_id+1, params_grid[restore_id]\n",
    "    nth_string = p.ordinal(param_id)\n",
    "    print(\"============={0} Hyper-Parameter==============\".format(nth_string))\n",
    "    pprint(params)\n",
    "    print(\"============={0} Hyper-Parameter==============\".format(nth_string))\n",
    "\n",
    "    checkpoint_filepath = \"./checkpoints/{0}/model_epoch{1}_step_{2}.ckpt\".format(nth_string,epoch, step)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    chosen_logits = build_logit(x, params, training=False)\n",
    "    loss = build_loss(y, chosen_logits, params)\n",
    "\n",
    "    # saver\n",
    "    saver = tf.train.Saver(max_to_keep=25)\n",
    "\n",
    "    # For testing accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(chosen_logits,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver.restore(sess, checkpoint_filepath)\n",
    "    curr_acc, test_loss = sess.run([accuracy, loss], feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Accuracy : {0:4.3f}%\".format(curr_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 모델을 선택하세요.\n",
    "\n",
    "Checkpoint 파일이 존재하는지 한번 더 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_id, epoch, step = 1, 0, 200\n",
    "restore_and_test(restore_id, epoch, step, params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
